{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이미지를 VAE로 Embedding하여 얻은 vector의 similarity와 풍속데이터의 Similarity 비교\n",
    "* 체크해야 할 것\n",
    "    - 풍속기록이 있는 데이터만 필요하며, 일부는 Train 일부는 test 이를 구분해야 할 필요가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader, TensorDataset\n",
    "\n",
    "from src.hillshpaeDataModule import MySimCLRTrainDataTransform\n",
    "from simCLR import *\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind data load\n",
    "\n",
    "with open('../../Share_Data/windspeed/_Main_dataset.pickle', 'rb') as f:\n",
    "    windspeed_dataset = pickle.load(f)\n",
    "\n",
    "# --- filter station for WAS journal!!\n",
    "with open('../../Share_Data/windspeed/WAS_except_station_list_under_30_m.pickle', 'rb') as f:\n",
    "    except_list = pickle.load(f)\n",
    "\n",
    "new_windspeed_dataset = {}\n",
    "for key in windspeed_dataset:\n",
    "    if key not in except_list:\n",
    "        new_windspeed_dataset[key] = windspeed_dataset[key]\n",
    "        \n",
    "windspeed_dataset = new_windspeed_dataset\n",
    "# -------------------------------------- #\n",
    "    \n",
    "station_list = list(windspeed_dataset.keys())\n",
    "windspeed_list = []\n",
    "for item in windspeed_dataset.values():\n",
    "    windspeed_list.append(item[3])\n",
    "windspeed_list = np.array(windspeed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "---\n",
    "\n",
    "[Experiment_1](#Experiment_1) - 위치기반 SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment_1 \n",
    "* 위치기반하여 SVR\n",
    "* k-hold-vaildation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거리로만\n",
    "from IPython.display import clear_output\n",
    "\n",
    "X = np.array(list(windspeed_dataset.values()))[:,:2]\n",
    "windspeed_list = windspeed_list.reshape(-1)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list =[]\n",
    "ans_list = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = windspeed_list[train_index], windspeed_list[test_index]\n",
    "    \n",
    "    regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
    "    regr.fit(X_train, Y_train)\n",
    "    result_list.append(regr.predict(X_test))\n",
    "    ans_list.append(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = list(map(lambda x : result_list[x]-ans_list[x], np.arange(kf.n_splits)))\n",
    "total_error = list(map(lambda x : np.mean(np.abs(x)), error_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of error : 3.301263627858107\n"
     ]
    }
   ],
   "source": [
    "print('mean of error : {}'.format(np.mean(total_error)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 생각보다 잘 나오긴하는데\n",
    "* 해당 Task는 굳이 NN을 사용하지 않아도 되지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "\n",
    "lr = 5e-4\n",
    "epochs = 5000\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "normal_x = StandardScaler().fit_transform(X)\n",
    "log_y = np.log(windspeed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_list = []\n",
    "nn_ans_list = []\n",
    "\n",
    "#loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "for train_index, test_index in kf.split(normal_x):\n",
    "    X_train = torch.tensor(normal_x[train_index], dtype=torch.float32)\n",
    "    X_test = torch.tensor(normal_x[test_index], dtype=torch.float32)\n",
    "    Y_train = torch.tensor(windspeed_list[train_index].reshape(-1,1), dtype=torch.float32)\n",
    "    Y_test = torch.tensor(windspeed_list[test_index].reshape(-1,1), dtype=torch.float32)\n",
    "    \n",
    "    #model reset\n",
    "    model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32,32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32,1))\n",
    "    \n",
    "    #Opt\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    schedulr = torch.optim.lr_scheduler.ReduceLROnPlateau(opt)\n",
    "    \n",
    "    for epoch in np.arange(epochs):\n",
    "        y_pred = model(X_train)\n",
    "        loss = loss_fn(y_pred, Y_train)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        schedulr.step(loss)\n",
    "        if epoch % 100 == 0:\n",
    "            print('step : {}, loss : {}'.format(epoch, loss.item()/len(y_pred)))\n",
    "    \n",
    "    print('---- training end ----')\n",
    "    y_pred = model(X_test)\n",
    "    loss = loss_fn(y_pred, Y_test)\n",
    "    print('val loss : {}'.format(loss.item()))\n",
    "    nn_list.append(y_pred.detach().numpy())\n",
    "    nn_ans_list.append(Y_test.detach().numpy())\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.7572541, 3.344172, 2.216308, 3.5453944, 3.5181408, 2.822469, 2.7147949, 4.159629, 4.54025, 3.9485848]\n",
      "3.4566998\n"
     ]
    }
   ],
   "source": [
    "result = list(map(lambda x : np.mean(np.abs(nn_ans_list[x] - nn_list[x])), np.arange(len(nn_ans_list))))\n",
    "print(result)\n",
    "print(np.mean(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.3729942, 3.5524478, 2.3723252, 2.6969068, 2.8846362, 3.3595045, 2.7867486, 4.1057005, 4.5895967, 4.1195188]\n",
      "3.3840377\n"
     ]
    }
   ],
   "source": [
    "result = list(map(lambda x : np.mean(np.abs(nn_ans_list[x] - nn_list[x])), np.arange(len(nn_ans_list))))\n",
    "print(result)\n",
    "print(np.mean(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.47251, 3.5159757, 2.325559, 2.9339614, 3.0844433, 3.176426, 2.8345044, 4.050174, 4.5520773, 4.229054]\n",
      "3.4174685\n"
     ]
    }
   ],
   "source": [
    "result = list(map(lambda x : np.mean(np.abs(nn_ans_list[x] - nn_list[x])), np.arange(len(nn_ans_list))))\n",
    "print(result)\n",
    "print(np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN \n",
    "\n",
    "* with latent vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_hill = np.load('data/WAS_hill_latent.npy')\n",
    "latent_rgb = np.load('data/WAS_RGB_latent.npy')\n",
    "latent_cart = np.load('data/WAS_CART_latent.npy')\n",
    "\n",
    "latent_all = np.concatenate([latent_rgb, latent_hill, latent_hill], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "# Hyperparameter\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "#latent = model_projection/model_n\n",
    "normal_x = np.concatenate([StandardScaler().fit_transform(X), latent_all], axis=-1)\n",
    "log_y = np.log(windspeed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(normal_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list =[]\n",
    "ans_list = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = normal_x[train_index], normal_x[test_index]\n",
    "    Y_train, Y_test = windspeed_list[train_index], windspeed_list[test_index]\n",
    "    \n",
    "    #regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
    "    reg = SVR(C=1.0, epsilon=0.2)\n",
    "    reg.fit(X_train, Y_train)\n",
    "    result_list.append(reg.predict(X_test))\n",
    "    ans_list.append(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = list(map(lambda x : result_list[x]-ans_list[x], np.arange(kf.n_splits)))\n",
    "total_error = list(map(lambda x : np.mean(np.abs(x)), error_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of error : 3.2148897180598617\n"
     ]
    }
   ],
   "source": [
    "print('mean of error : {}'.format(np.mean(total_error)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "\n",
    "lr = 1e-4\n",
    "epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "total_result = []\n",
    "nn_list = []\n",
    "nn_ans_list = []\n",
    "\n",
    "#loss function\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "for train_index, test_index in kf.split(normal_x):\n",
    "    X_train = torch.tensor(normal_x[train_index], dtype=torch.float32)\n",
    "    X_test = torch.tensor(normal_x[test_index], dtype=torch.float32)\n",
    "#    Y_train = torch.tensor(log_y[train_index].reshape(-1,1), dtype=torch.float32)\n",
    "#    Y_test = torch.tensor(log_y[test_index].reshape(-1,1), dtype=torch.float32)\n",
    "    Y_train = torch.tensor(windspeed_list[train_index].reshape(-1,1), dtype=torch.float32)\n",
    "    Y_test = torch.tensor(windspeed_list[test_index].reshape(-1,1), dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "    #model reset\n",
    "    model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2+32*3, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024,1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024,1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024,512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512,1))\n",
    "\n",
    "    #Opt\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    schedulr = torch.optim.lr_scheduler.ReduceLROnPlateau(opt)\n",
    "    \n",
    "    temp_train_loss = []\n",
    "    temp_test_loss = []\n",
    "    \n",
    "    for epoch in np.arange(epochs):\n",
    "        y_pred = model(X_train)\n",
    "        loss = loss_fn(y_pred, Y_train)\n",
    "        temp_train_loss.append(float(loss.data))\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        schedulr.step(loss)\n",
    "        \n",
    "        y_pred = model(X_test)\n",
    "        loss = loss_fn(y_pred, Y_test)\n",
    "        temp_test_loss.append(float(loss.data))\n",
    "        \n",
    "    train_loss.append(temp_train_loss)\n",
    "    val_loss.append(temp_test_loss)\n",
    "    \n",
    "#    nn_list.append(np.exp(y_pred.detach().numpy()))\n",
    "#    nn_ans_list.append(np.exp(Y_test.detach().numpy()))\n",
    "    nn_list.append(y_pred.detach().numpy())\n",
    "    nn_ans_list.append(Y_test.detach().numpy())\n",
    "    total_result.append(np.mean(list(map(lambda x : np.mean(np.abs(nn_ans_list[x] - nn_list[x])), np.arange(len(nn_ans_list))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(train_loss)):\n",
    "#    plt.plot(np.arange(len(train_loss[i])), train_loss[i])\n",
    "    plt.plot(np.arange(len(val_loss[i]))[50:], val_loss[i][50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_result)\n",
    "print(np.mean(total_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'WAS_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(34, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024,1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024,1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024,512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.load_state_dict(torch.load('WAS_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.312671, 3.347396, 3.160229, 3.1332514, 3.0017517, 3.1042824, 3.0894096, 3.1937537, 3.4063134, 3.4999619]\n",
      "3.2249017\n"
     ]
    }
   ],
   "source": [
    "print(total_result)\n",
    "print(np.mean(total_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 올...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch16",
   "language": "python",
   "name": "pytorch16"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
